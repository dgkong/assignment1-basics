{
    "train_data": "data/ts_train_tokenized.npy",
    "val_data": "data/ts_val_tokenized.npy",

    "model": {
        "vocab_size": 10000,
        "context_length": 256,
        "num_layers": 4,
        "d_model": 512,
        "num_heads": 16,
        "d_ff": 1344,
        "rope_theta": 1e4
    },

    "optimizer": {
        "lr": 1e-3,
        "betas": [0.9, 0.999],
        "eps": 1e-8,
        "weight_decay": 1e-2
    },
    
    "total_tokens": 327680000,
    "batch_size": 8,
    "max_steps": 160000,
    "warmup_iters": 8000,
    "max_lr": 3e-3,
    "min_lr": 0,
    "max_l2_norm": 1.0,
    "val_interval": 1000,
    "val_steps": 100,
    "checkpoint_interval": 5000
}
