{
    "train_data": "data/ts_train_tokenized.npy",
    "val_data": "data/ts_val_tokenized.npy",

    "model": {
        "vocab_size": 10000,
        "context_length": 256,
        "num_layers": 4,
        "d_model": 512,
        "num_heads": 16,
        "d_ff": 1344,
        "rope_theta": 1e4
    },

    "optimizer": {
        "lr": 1e-3,
        "betas": [0.9, 0.95],
        "eps": 1e-8,
        "weight_decay": 1e-2
    },
    
    "total_tokens": 40960000,
    "total_batch": 64,
    "batch_size": 8,
    "warmup_iter_ratio": 0.05,
    "max_lr": 1e-3,
    "min_lr": 0,
    "max_l2_norm": 1.0,
    "val_interval_ratio": 0.1,
    "val_steps": 250,
    "checkpoint_interval_ratio": 0.25
}
